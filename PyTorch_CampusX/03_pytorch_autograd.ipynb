{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPgSV/gJw/Hzz4LDURgyBi9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **PyTorch Autograd**\n","\n","Autograd is PyTorch's automatic differentiation engine. It keeps track of all operations performed on tensors that have `requires_grad=True`, creating a computation graph dynamically. This graph is used to compute gradients for optimization tasks like backpropagation.\n","\n","### **How Does It Work?**\n","1. **Computation Graph:**\n","   - When you perform operations on tensors, autograd dynamically builds a directed acyclic graph (DAG) where nodes represent operations and edges represent the flow of data.\n","   - This graph allows autograd to trace how each tensor is derived from others.\n","\n","2. **Backward Pass:**\n","   - When you call `.backward()` on a tensor, autograd traverses the graph in reverse order (hence \"backpropagation\"), computing gradients for all tensors with `requires_grad=True`.\n","\n","3. **Gradient Storage:**\n","   - Gradients are stored in the `.grad` attribute of the corresponding tensor."],"metadata":{"id":"hxp2jwIqbI9S"}},{"cell_type":"markdown","source":["## **Import Dependencies**"],"metadata":{"id":"6-eaB_SKkMZB"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"R4cOlDb_kQwD","executionInfo":{"status":"ok","timestamp":1737239751080,"user_tz":-60,"elapsed":4689,"user":{"displayName":"Krishnagopal Halder","userId":"16954898871344510854"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## **Calculate Gradient Manually**\n","\n","**Example-1:**<br>\n","We want to calculate the gradient of the function:\n","\n","$$ y = x^2 $$\n","\n","The derivative of \\( y \\) with respect to \\( x \\) is:\n","\n","$$ \\frac{\\partial y}{\\partial x} = 2x $$\n","\n","Using Python, we can compute the gradient at a specific value of \\( x \\) with the following code:\n"],"metadata":{"id":"O86hSGhidee6"}},{"cell_type":"code","source":["# Function to calculate the gradient of y = x^2\n","def dy_dx(x):\n","    \"\"\"\n","    Calculate the derivative of y = x^2 with respect to x.\n","\n","    Parameters:\n","        x (float or int): The value of x at which the gradient is evaluated.\n","\n","    Returns:\n","        float: The gradient (2 * x).\n","    \"\"\"\n","    return 2 * x\n","\n","# Example usage\n","x = 3\n","gradient = dy_dx(x)\n","print(f\"The gradient of y = x^2 at x = {x} is {gradient}.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z3Gig1uUjqGv","executionInfo":{"status":"ok","timestamp":1737239751080,"user_tz":-60,"elapsed":6,"user":{"displayName":"Krishnagopal Halder","userId":"16954898871344510854"}},"outputId":"64b47f94-1056-4261-dd5b-c7f5da02eec0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["The gradient of y = x^2 at x = 3 is 6.\n"]}]},{"cell_type":"markdown","source":["**Example-2:**<br>\n","We want to calculate the gradient of the function:\n","$$ y = x^2 $$\n","$$ z = \\sin {y} $$\n","\n","The derivative of \\( z \\) with respect to \\( x \\) is:\n","\n","$$ \\frac{\\partial z}{\\partial x} = \\frac{∂z}{∂y} ⋅ \\frac{∂y}{∂x} $$\n","$$ \\frac{\\partial z}{\\partial x} = \\cos{y} ⋅ 2x $$\n","$$ \\frac{\\partial z}{\\partial x} = \\cos{x^2} ⋅ 2x $$\n","\n","Using Python, we can compute the gradient at a specific value of \\( x \\) with the following code:"],"metadata":{"id":"pB2PBqDso1BR"}},{"cell_type":"code","source":["# Function to calculate the gradient of z\n","import math\n","\n","def dz_dx(x):\n","    \"\"\"\n","    Calculate the derivative of z with respect to x.\n","\n","    Parameters:\n","        y (float or int): The value of x at which the gradient is evaluated.\n","\n","    Returns:\n","        float: The gradient.\n","    \"\"\"\n","    return math.cos(x**2) * (2 * x)\n","\n","# Example usage\n","x = 3\n","gradient = dz_dx(x)\n","print(f\"The gradient of z at x = {x} is {gradient:.2f}.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SNefBd91pTt2","executionInfo":{"status":"ok","timestamp":1737239751080,"user_tz":-60,"elapsed":5,"user":{"displayName":"Krishnagopal Halder","userId":"16954898871344510854"}},"outputId":"4453ec90-eace-482c-dd23-2b6de822db66"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["The gradient of z at x = 3 is -5.47.\n"]}]},{"cell_type":"markdown","source":["## **Calculate Gradient using PyTorch**"],"metadata":{"id":"6XAw9PBatKqg"}},{"cell_type":"code","source":["# Example-1\n","# Define a tensor with gradient tracking enabled\n","x = torch.tensor(3.0, requires_grad=True)\n","\n","# Define the function y = x^2\n","y = x**2\n","\n","# Print the values of x and y\n","print(\"x:\", x)\n","print(\"y:\", y)\n","\n","# Perform backpropagation to compute the gradient\n","y.backward()\n","\n","# Print the gradient of y with respect to x\n","print(\"Gradient (dy/dx):\", x.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TjupWz-4tQSn","executionInfo":{"status":"ok","timestamp":1737239751080,"user_tz":-60,"elapsed":4,"user":{"displayName":"Krishnagopal Halder","userId":"16954898871344510854"}},"outputId":"b9333d21-a682-4de5-86c8-13c99993a5ab"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["x: tensor(3., requires_grad=True)\n","y: tensor(9., grad_fn=<PowBackward0>)\n","Gradient (dy/dx): tensor(6.)\n"]}]},{"cell_type":"code","source":["# Example-2\n","# Define a tensor with gradient tracking enabled\n","x = torch.tensor(3.0, requires_grad=True)\n","\n","# Define the function y = x^2\n","y = x**2\n","\n","# Define the function z = sin(y)\n","z = torch.sin(y)\n","\n","# Print the values of x and y\n","print(\"x:\", x)\n","print(\"y:\", y)\n","print(\"z:\", z)\n","\n","# Perform backpropagation to compute the gradient\n","z.backward()\n","\n","# Print the gradient of y with respect to x\n","print(\"Gradient (dy/dx):\", x.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0is1TmcVt_vP","executionInfo":{"status":"ok","timestamp":1737239751080,"user_tz":-60,"elapsed":3,"user":{"displayName":"Krishnagopal Halder","userId":"16954898871344510854"}},"outputId":"4f6fa99c-341f-46c1-abff-958d4123cd48"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["x: tensor(3., requires_grad=True)\n","y: tensor(9., grad_fn=<PowBackward0>)\n","z: tensor(0.4121, grad_fn=<SinBackward0>)\n","Gradient (dy/dx): tensor(-5.4668)\n"]}]},{"cell_type":"markdown","source":["## **Manual Gradient of Loss Calculation w.r.t Weight and Bias**\n","\n","1. Linear Transformation:\n","$$ z = w \\cdot x + b $$\n","2. Activation:\n","$$ y_{pred} = σ(z) = \\frac{1}{1 + e^{-z}} $$\n","3. Loss Function (Binary Cross-Entropy Loss):\n","$$ L = -[y_{target} \\cdot \\ln(y_{pred}) + (1 - y_{target}) \\cdot \\ln( - y_{pred})] $$\n"],"metadata":{"id":"hnzg-HZskgJ2"}},{"cell_type":"code","source":["# Inputs\n","x = torch.tensor(6.7) # Input feature\n","y = torch.tensor(0.0) # True Label (Binary)\n","\n","w = torch.tensor(1.0) # Weight\n","b = torch.tensor(0.0) # Bias"],"metadata":{"id":"F9rVMl9OkmOO","executionInfo":{"status":"ok","timestamp":1737240113448,"user_tz":-60,"elapsed":218,"user":{"displayName":"Krishnagopal Halder","userId":"16954898871344510854"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Binary Cross-Entropy Loss for scalar\n","def binary_cross_entropy_loss(prediction, target):\n","    epsilon = 1e-8\n","    prediction = torch.clamp(prediction, epsilon, 1-epsilon)\n","    return -(target * torch.log(prediction) + (1 - target) * torch.log(1 - prediction))"],"metadata":{"id":"Id8yFk5TlHRh","executionInfo":{"status":"ok","timestamp":1737240941033,"user_tz":-60,"elapsed":208,"user":{"displayName":"Krishnagopal Halder","userId":"16954898871344510854"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Forward pass\n","z = w * x + b # Weighted sum (Linear Transformation)\n","y_pred = torch.sigmoid(z) # Predicted Probability (Activation)\n","\n","# Compute binary cross-entropy loss\n","loss = binary_cross_entropy_loss(y_pred, y)\n","print(loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M317Df_uoTNo","executionInfo":{"status":"ok","timestamp":1737241262860,"user_tz":-60,"elapsed":220,"user":{"displayName":"Krishnagopal Halder","userId":"16954898871344510854"}},"outputId":"da8f6ea1-5590-4de8-9bb5-143ff5721442"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(6.7012)\n"]}]},{"cell_type":"code","source":["# Derivatives:\n","# 1. dL/d(y_pred): Loss with respect to the prediction (y_pred)\n","dloss_dy_pred = (y_pred - y) / (y_pred * (1 - y_pred))\n","\n","# 2. d(y_pred)/dz: Prediction (y_pred) with respect to z (sigmoid derivative)\n","dy_pred_dz = y_pred * (1 - y_pred)\n","\n","# 3. dz/dw and dz/db: z with respect to w and b\n","dz_dw = x # dz/dw = x\n","dz_db = 1 # dz/db = 1 (bias contributes directly to z)\n","\n","dL_dw = dloss_dy_pred * dy_pred_dz * dz_dw\n","dL_db = dloss_dy_pred * dy_pred_dz * dz_db\n","print(f\"Manual Gradient of loss w.r.t weight (dw): {dL_dw:.4f}\")\n","print(f\"Manual Gradient of loss w.r.t bias (db): {dL_db:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ZbT9UsOopoq","executionInfo":{"status":"ok","timestamp":1737241511871,"user_tz":-60,"elapsed":3,"user":{"displayName":"Krishnagopal Halder","userId":"16954898871344510854"}},"outputId":"e68ee04d-ad31-41a3-ddeb-22cd45ee26e7"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Manual Gradient of loss w.r.t weight (dw): 6.6918\n","Manual Gradient of loss w.r.t bias (db): 0.9988\n"]}]},{"cell_type":"markdown","source":["## **Automatic Gradient of Loss Calculation w.r.t Weight and Bias using Autograd**"],"metadata":{"id":"a2fQoQ2Booqy"}},{"cell_type":"code","source":["# Inputs\n","x = torch.tensor(6.7) # Input feature\n","y = torch.tensor(0.0) # True Label (Binary)\n","\n","w = torch.tensor(1.0, requires_grad=True) # Weight\n","b = torch.tensor(0.0, requires_grad=True) # Bias"],"metadata":{"id":"kqNYJc2PqrSw","executionInfo":{"status":"ok","timestamp":1737241624557,"user_tz":-60,"elapsed":2,"user":{"displayName":"Krishnagopal Halder","userId":"16954898871344510854"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Forward pass\n","z = w * x + b # Weighted sum (Linear Transformation)\n","y_pred = torch.sigmoid(z) # Predicted Probability (Activation)\n","\n","# Compute binary cross-entropy loss\n","loss = binary_cross_entropy_loss(y_pred, y)\n","print(loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wbeE6290q6hS","executionInfo":{"status":"ok","timestamp":1737241655224,"user_tz":-60,"elapsed":765,"user":{"displayName":"Krishnagopal Halder","userId":"16954898871344510854"}},"outputId":"ba92e823-d93f-40d4-fdb6-77430c2d0577"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(6.7012, grad_fn=<NegBackward0>)\n"]}]},{"cell_type":"code","source":["loss.backward()\n","\n","print(w.grad)\n","print(b.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2h5JdvgYrJNM","executionInfo":{"status":"ok","timestamp":1737241720252,"user_tz":-60,"elapsed":3,"user":{"displayName":"Krishnagopal Halder","userId":"16954898871344510854"}},"outputId":"38c41093-495b-4ad2-f3e5-84be8666412f"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(6.6918)\n","tensor(0.9988)\n"]}]},{"cell_type":"markdown","source":["## **Calculate Gradients for Multiple Inputs**"],"metadata":{"id":"v4o-9XY8srX1"}},{"cell_type":"code","source":["# Create a PyTorch tensor with multiple inputs\n","x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n","print(x)\n","y = (x ** 2).mean()\n","print(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kjvJMULTs4Tn","executionInfo":{"status":"ok","timestamp":1737242652322,"user_tz":-60,"elapsed":257,"user":{"displayName":"Krishnagopal Halder","userId":"16954898871344510854"}},"outputId":"16b29523-b469-4361-ca9f-268d686ad11c"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1., 2., 3.], requires_grad=True)\n","tensor(4.6667, grad_fn=<MeanBackward0>)\n"]}]},{"cell_type":"code","source":["y.backward()\n","x.grad"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PR2RB3AZtPAa","executionInfo":{"status":"ok","timestamp":1737242653175,"user_tz":-60,"elapsed":250,"user":{"displayName":"Krishnagopal Halder","userId":"16954898871344510854"}},"outputId":"17851f38-c10b-4eaa-833a-fd8b9b0c9721"},"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.6667, 1.3333, 2.0000])"]},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","source":["## **Clearing Gradients**\n","Gradients can be cleared using the `optimizer.zero_grad()` function when using optimizers. For manually tracking gradients, you can reset the gradients by assigning None to the .grad attribute of the tensor."],"metadata":{"id":"C5rHyDtRtlRr"}},{"cell_type":"code","source":["x = torch.tensor(6.0, requires_grad=True)\n","y = (x ** 2)"],"metadata":{"id":"a-0-Z1mUt1c2","executionInfo":{"status":"ok","timestamp":1737242656088,"user_tz":-60,"elapsed":234,"user":{"displayName":"Krishnagopal Halder","userId":"16954898871344510854"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["y.backward()\n","print(x.grad)\n","x.grad.zero_()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6DQp4tX7uRU5","executionInfo":{"status":"ok","timestamp":1737242657604,"user_tz":-60,"elapsed":211,"user":{"displayName":"Krishnagopal Halder","userId":"16954898871344510854"}},"outputId":"48edb965-965c-44e4-fec0-f00a8bc92f63"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(12.)\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor(0.)"]},"metadata":{},"execution_count":41}]},{"cell_type":"markdown","source":["## **Disable Gradient Tracking**\n","n PyTorch, you can disable gradient tracking when gradients are not needed, such as during inference or evaluations, to improve computational efficiency. This is done using the `torch.no_grad() `context manager or by setting `requires_grad=False` for specific tensors."],"metadata":{"id":"OeAwfEcTvC_X"}},{"cell_type":"code","source":["# Create tensors with gradient tracking enabled\n","x = torch.tensor(6.7, requires_grad=True)\n","w = torch.tensor(1.0, requires_grad=True)\n","b = torch.tensor(0.0, requires_grad=True)\n","\n","# Perform operations without gradient tracking\n","with torch.no_grad():\n","    z = w * x + b  # No gradients will be tracked for this operation\n","    y_pred = torch.sigmoid(z)\n","\n","print(f\"z: {z}\")\n","print(f\"y_pred: {y_pred}\")\n","\n","# Verify that gradients are not tracked\n","print(f\"Requires Grad (z): {z.requires_grad}\")  # Output: False"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xEsyVP6rvHXn","executionInfo":{"status":"ok","timestamp":1737242857201,"user_tz":-60,"elapsed":252,"user":{"displayName":"Krishnagopal Halder","userId":"16954898871344510854"}},"outputId":"76743d51-0de3-4b3f-9160-6c264c6a8a61"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["z: 6.699999809265137\n","y_pred: 0.998770534992218\n","Requires Grad (z): False\n"]}]}]}