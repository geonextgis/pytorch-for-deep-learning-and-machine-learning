{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxp2jwIqbI9S"
   },
   "source": [
    "# **PyTorch Autograd**\n",
    "\n",
    "Autograd is PyTorch's automatic differentiation engine. It keeps track of all operations performed on tensors that have `requires_grad=True`, creating a computation graph dynamically. This graph is used to compute gradients for optimization tasks like backpropagation.\n",
    "\n",
    "### **How Does It Work?**\n",
    "1. **Computation Graph:**\n",
    "   - When you perform operations on tensors, autograd dynamically builds a directed acyclic graph (DAG) where nodes represent operations and edges represent the flow of data.\n",
    "   - This graph allows autograd to trace how each tensor is derived from others.\n",
    "\n",
    "2. **Backward Pass:**\n",
    "   - When you call `.backward()` on a tensor, autograd traverses the graph in reverse order (hence \"backpropagation\"), computing gradients for all tensors with `requires_grad=True`.\n",
    "\n",
    "3. **Gradient Storage:**\n",
    "   - Gradients are stored in the `.grad` attribute of the corresponding tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-eaB_SKkMZB"
   },
   "source": [
    "## **Import Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4689,
     "status": "ok",
     "timestamp": 1737239751080,
     "user": {
      "displayName": "Krishnagopal Halder",
      "userId": "16954898871344510854"
     },
     "user_tz": -60
    },
    "id": "R4cOlDb_kQwD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O86hSGhidee6"
   },
   "source": [
    "## **Calculate Gradient Manually**\n",
    "\n",
    "**Example-1:**<br>\n",
    "We want to calculate the gradient of the function:\n",
    "\n",
    "$$ y = x^2 $$\n",
    "\n",
    "The derivative of \\( y \\) with respect to \\( x \\) is:\n",
    "\n",
    "$$ \\frac{\\partial y}{\\partial x} = 2x $$\n",
    "\n",
    "Using Python, we can compute the gradient at a specific value of \\( x \\) with the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1737239751080,
     "user": {
      "displayName": "Krishnagopal Halder",
      "userId": "16954898871344510854"
     },
     "user_tz": -60
    },
    "id": "z3Gig1uUjqGv",
    "outputId": "64b47f94-1056-4261-dd5b-c7f5da02eec0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient of y = x^2 at x = 3 is 6.\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate the gradient of y = x^2\n",
    "def dy_dx(x):\n",
    "    \"\"\"\n",
    "    Calculate the derivative of y = x^2 with respect to x.\n",
    "\n",
    "    Parameters:\n",
    "        x (float or int): The value of x at which the gradient is evaluated.\n",
    "\n",
    "    Returns:\n",
    "        float: The gradient (2 * x).\n",
    "    \"\"\"\n",
    "    return 2 * x\n",
    "\n",
    "# Example usage\n",
    "x = 3\n",
    "gradient = dy_dx(x)\n",
    "print(f\"The gradient of y = x^2 at x = {x} is {gradient}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pB2PBqDso1BR"
   },
   "source": [
    "**Example-2:**<br>\n",
    "We want to calculate the gradient of the function:\n",
    "$$ y = x^2 $$\n",
    "$$ z = \\sin {y} $$\n",
    "\n",
    "The derivative of \\( z \\) with respect to \\( x \\) is:\n",
    "\n",
    "$$ \\frac{\\partial z}{\\partial x} = \\frac{∂z}{∂y} ⋅ \\frac{∂y}{∂x} $$\n",
    "$$ \\frac{\\partial z}{\\partial x} = \\cos{y} ⋅ 2x $$\n",
    "$$ \\frac{\\partial z}{\\partial x} = \\cos{x^2} ⋅ 2x $$\n",
    "\n",
    "Using Python, we can compute the gradient at a specific value of \\( x \\) with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1737239751080,
     "user": {
      "displayName": "Krishnagopal Halder",
      "userId": "16954898871344510854"
     },
     "user_tz": -60
    },
    "id": "SNefBd91pTt2",
    "outputId": "4453ec90-eace-482c-dd23-2b6de822db66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient of z at x = 3 is -5.47.\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate the gradient of z\n",
    "import math\n",
    "\n",
    "def dz_dx(x):\n",
    "    \"\"\"\n",
    "    Calculate the derivative of z with respect to x.\n",
    "\n",
    "    Parameters:\n",
    "        y (float or int): The value of x at which the gradient is evaluated.\n",
    "\n",
    "    Returns:\n",
    "        float: The gradient.\n",
    "    \"\"\"\n",
    "    return math.cos(x**2) * (2 * x)\n",
    "\n",
    "# Example usage\n",
    "x = 3\n",
    "gradient = dz_dx(x)\n",
    "print(f\"The gradient of z at x = {x} is {gradient:.2f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XAw9PBatKqg"
   },
   "source": [
    "## **Calculate Gradient using PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1737239751080,
     "user": {
      "displayName": "Krishnagopal Halder",
      "userId": "16954898871344510854"
     },
     "user_tz": -60
    },
    "id": "TjupWz-4tQSn",
    "outputId": "b9333d21-a682-4de5-86c8-13c99993a5ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor(3., requires_grad=True)\n",
      "y: tensor(9., grad_fn=<PowBackward0>)\n",
      "Gradient (dy/dx): tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "# Example-1\n",
    "# Define a tensor with gradient tracking enabled\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Define the function y = x^2\n",
    "y = x**2\n",
    "\n",
    "# Print the values of x and y\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)\n",
    "\n",
    "# Perform backpropagation to compute the gradient\n",
    "y.backward()\n",
    "\n",
    "# Print the gradient of y with respect to x\n",
    "print(\"Gradient (dy/dx):\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1737239751080,
     "user": {
      "displayName": "Krishnagopal Halder",
      "userId": "16954898871344510854"
     },
     "user_tz": -60
    },
    "id": "0is1TmcVt_vP",
    "outputId": "4f6fa99c-341f-46c1-abff-958d4123cd48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor(3., requires_grad=True)\n",
      "y: tensor(9., grad_fn=<PowBackward0>)\n",
      "z: tensor(0.4121, grad_fn=<SinBackward0>)\n",
      "Gradient (dy/dx): tensor(-5.4668)\n"
     ]
    }
   ],
   "source": [
    "# Example-2\n",
    "# Define a tensor with gradient tracking enabled\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Define the function y = x^2\n",
    "y = x**2\n",
    "\n",
    "# Define the function z = sin(y)\n",
    "z = torch.sin(y)\n",
    "\n",
    "# Print the values of x and y\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)\n",
    "print(\"z:\", z)\n",
    "\n",
    "# Perform backpropagation to compute the gradient\n",
    "z.backward()\n",
    "\n",
    "# Print the gradient of y with respect to x\n",
    "print(\"Gradient (dy/dx):\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnzg-HZskgJ2"
   },
   "source": [
    "## **Manual Gradient of Loss Calculation w.r.t Weight and Bias**\n",
    "\n",
    "1. Linear Transformation:\n",
    "$$ z = w \\cdot x + b $$\n",
    "2. Activation:\n",
    "$$ y_{pred} = σ(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "3. Loss Function (Binary Cross-Entropy Loss):\n",
    "$$ L = -[y_{target} \\cdot \\ln(y_{pred}) + (1 - y_{target}) \\cdot \\ln( - y_{pred})] $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 218,
     "status": "ok",
     "timestamp": 1737240113448,
     "user": {
      "displayName": "Krishnagopal Halder",
      "userId": "16954898871344510854"
     },
     "user_tz": -60
    },
    "id": "F9rVMl9OkmOO"
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "x = torch.tensor(6.7) # Input feature\n",
    "y = torch.tensor(0.0) # True Label (Binary)\n",
    "\n",
    "w = torch.tensor(1.0) # Weight\n",
    "b = torch.tensor(0.0) # Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 208,
     "status": "ok",
     "timestamp": 1737240941033,
     "user": {
      "displayName": "Krishnagopal Halder",
      "userId": "16954898871344510854"
     },
     "user_tz": -60
    },
    "id": "Id8yFk5TlHRh"
   },
   "outputs": [],
   "source": [
    "# Binary Cross-Entropy Loss for scalar\n",
    "def binary_cross_entropy_loss(prediction, target):\n",
    "    epsilon = 1e-8\n",
    "    prediction = torch.clamp(prediction, epsilon, 1-epsilon)\n",
    "    return -(target * torch.log(prediction) + (1 - target) * torch.log(1 - prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1737241262860,
     "user": {
      "displayName": "Krishnagopal Halder",
      "userId": "16954898871344510854"
     },
     "user_tz": -60
    },
    "id": "M317Df_uoTNo",
    "outputId": "da8f6ea1-5590-4de8-9bb5-143ff5721442"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.7012)\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "z = w * x + b # Weighted sum (Linear Transformation)\n",
    "y_pred = torch.sigmoid(z) # Predicted Probability (Activation)\n",
    "\n",
    "# Compute binary cross-entropy loss\n",
    "loss = binary_cross_entropy_loss(y_pred, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1737241511871,
     "user": {
      "displayName": "Krishnagopal Halder",
      "userId": "16954898871344510854"
     },
     "user_tz": -60
    },
    "id": "7ZbT9UsOopoq",
    "outputId": "e68ee04d-ad31-41a3-ddeb-22cd45ee26e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Gradient of loss w.r.t weight (dw): 6.6918\n",
      "Manual Gradient of loss w.r.t bias (db): 0.9988\n"
     ]
    }
   ],
   "source": [
    "# Derivatives:\n",
    "# 1. dL/d(y_pred): Loss with respect to the prediction (y_pred)\n",
    "dloss_dy_pred = (y_pred - y) / (y_pred * (1 - y_pred))\n",
    "\n",
    "# 2. d(y_pred)/dz: Prediction (y_pred) with respect to z (sigmoid derivative)\n",
    "dy_pred_dz = y_pred * (1 - y_pred)\n",
    "\n",
    "# 3. dz/dw and dz/db: z with respect to w and b\n",
    "dz_dw = x # dz/dw = x\n",
    "dz_db = 1 # dz/db = 1 (bias contributes directly to z)\n",
    "\n",
    "dL_dw = dloss_dy_pred * dy_pred_dz * dz_dw\n",
    "dL_db = dloss_dy_pred * dy_pred_dz * dz_db\n",
    "print(f\"Manual Gradient of loss w.r.t weight (dw): {dL_dw:.4f}\")\n",
    "print(f\"Manual Gradient of loss w.r.t bias (db): {dL_db:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2fQoQ2Booqy"
   },
   "source": [
    "## **Automatic Gradient of Loss Calculation w.r.t Weight and Bias using Autograd**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1737241624557,
     "user": {
      "displayName": "Krishnagopal Halder",
      "userId": "16954898871344510854"
     },
     "user_tz": -60
    },
    "id": "kqNYJc2PqrSw"
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "x = torch.tensor(6.7) # Input feature\n",
    "y = torch.tensor(0.0) # True Label (Binary)\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad=True) # Weight\n",
    "b = torch.tensor(0.0, requires_grad=True) # Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 765,
     "status": "ok",
     "timestamp": 1737241655224,
     "user": {
      "displayName": "Krishnagopal Halder",
      "userId": "16954898871344510854"
     },
     "user_tz": -60
    },
    "id": "wbeE6290q6hS",
    "outputId": "ba92e823-d93f-40d4-fdb6-77430c2d0577"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.7012, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "z = w * x + b # Weighted sum (Linear Transformation)\n",
    "y_pred = torch.sigmoid(z) # Predicted Probability (Activation)\n",
    "\n",
    "# Compute binary cross-entropy loss\n",
    "loss = binary_cross_entropy_loss(y_pred, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1737241720252,
     "user": {
      "displayName": "Krishnagopal Halder",
      "userId": "16954898871344510854"
     },
     "user_tz": -60
    },
    "id": "2h5JdvgYrJNM",
    "outputId": "38c41093-495b-4ad2-f3e5-84be8666412f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.6918)\n",
      "tensor(0.9988)\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4o-9XY8srX1"
   },
   "source": [
    "## **Calculate Gradients for Multiple Inputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1737242652322,
     "user": {
      "displayName": "Krishnagopal Halder",
      "userId": "16954898871344510854"
     },
     "user_tz": -60
    },
    "id": "kjvJMULTs4Tn",
    "outputId": "16b29523-b469-4361-ca9f-268d686ad11c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], requires_grad=True)\n",
      "tensor(4.6667, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create a PyTorch tensor with multiple inputs\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "print(x)\n",
    "y = (x ** 2).mean()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1737242653175,
     "user": {
      "displayName": "Krishnagopal Halder",
      "userId": "16954898871344510854"
     },
     "user_tz": -60
    },
    "id": "PR2RB3AZtPAa",
    "outputId": "17851f38-c10b-4eaa-833a-fd8b9b0c9721"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6667, 1.3333, 2.0000])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5rHyDtRtlRr"
   },
   "source": [
    "## **Clearing Gradients**\n",
    "Gradients can be cleared using the `optimizer.zero_grad()` function when using optimizers. For manually tracking gradients, you can reset the gradients by assigning None to the .grad attribute of the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 234,
     "status": "ok",
     "timestamp": 1737242656088,
     "user": {
      "displayName": "Krishnagopal Halder",
      "userId": "16954898871344510854"
     },
     "user_tz": -60
    },
    "id": "a-0-Z1mUt1c2"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(6.0, requires_grad=True)\n",
    "y = (x ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1737242657604,
     "user": {
      "displayName": "Krishnagopal Halder",
      "userId": "16954898871344510854"
     },
     "user_tz": -60
    },
    "id": "6DQp4tX7uRU5",
    "outputId": "48edb965-965c-44e4-fec0-f00a8bc92f63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "print(x.grad)\n",
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeAwfEcTvC_X"
   },
   "source": [
    "## **Disable Gradient Tracking**\n",
    "n PyTorch, you can disable gradient tracking when gradients are not needed, such as during inference or evaluations, to improve computational efficiency. This is done using the `torch.no_grad() `context manager or by setting `requires_grad=False` for specific tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1737242857201,
     "user": {
      "displayName": "Krishnagopal Halder",
      "userId": "16954898871344510854"
     },
     "user_tz": -60
    },
    "id": "xEsyVP6rvHXn",
    "outputId": "76743d51-0de3-4b3f-9160-6c264c6a8a61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z: 6.699999809265137\n",
      "y_pred: 0.998770534992218\n",
      "Requires Grad (z): False\n"
     ]
    }
   ],
   "source": [
    "# Create tensors with gradient tracking enabled\n",
    "x = torch.tensor(6.7, requires_grad=True)\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "# Perform operations without gradient tracking\n",
    "with torch.no_grad():\n",
    "    z = w * x + b  # No gradients will be tracked for this operation\n",
    "    y_pred = torch.sigmoid(z)\n",
    "\n",
    "print(f\"z: {z}\")\n",
    "print(f\"y_pred: {y_pred}\")\n",
    "\n",
    "# Verify that gradients are not tracked\n",
    "print(f\"Requires Grad (z): {z.requires_grad}\")  # Output: False"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPgSV/gJw/Hzz4LDURgyBi9",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
